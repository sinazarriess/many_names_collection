% experiments.tex

\cs{todo: discuss differences between domains}
\gbt{Not sure we'll have space :) }

\gbt{Note: I've rewritten this intro, the previous version with the plan of what is to be covered is commented out in the tex file}

We use \mn to gain insight into the extent to which current models account for object naming.
We do so in three ways: By analyzing the behavior of existing models on entry-level names, by assessing the consequences of fine-tuning models with entry-level name data, and by analyzing the predictions of the models, comparing them to human naming behavior as attested in \mn.
The latter will be done in Section~\ref{sec:analysis}; here we focus on modeling experiments.
\gbt{Maybe this needs to go in the intro or somewhere else. It's important to stress that the modeling experiments are on entry-level only, and the analysis takes into account the other names.}

The modeling experiments adopt the standard single-label object classification approach of Computer Vision, and we will compare prediction of the original \vg name with the entry-level name.
We leave modeling experiments that take into account other possible names of objects for future work.

% Our goal is to experimentally obtain insights into the usefulness of computer vision object labeling models, that are commonly used for \lv methods, to account for natural human object naming. 
% Specifically, our research questions are
% \begin{enumerate}
% 	\item Do object detection models, despite being trained on \arbitrary  
% 	names, account for human object naming?\cs{repetitive}
% 	\item Can we apply simple transfer learning on \mn to train classifiers towards this naming task by utilizing pre-trained object detection or image classification model as feature extractors? Figure~\ref{fig:exp_confusions} \\
% 	\cs{move to models: -- can we learn entry-level names by fine-tuning object detection models on \mn?\\
% 		-- Can we directly learn entry-level names by fine-tuning image classification models on \mn, i.e.,\ the pre-trained features to initialize object detection models?}
% \end{enumerate}
% TO BE COVERED BY THE SUBSECTIONS BELOW:
% \begin{enumerate}
% 	\item Do models prefer the entry-level name of an object over an \arbitrary name? Table~\ref{tab:exp_VGvsMN}
% 	\item In cases of failure, do they predict alternative, but less preferred  names instead? Tables~\ref{tab:exp_overview_results}, \ref{tab:exp_alternatives}
% 	\item What phenomena of human behavior on the object naming task of \mn do the models mirror? That is, which model mistakes or confusions are due to the artificial setup of the task itself, and less a failure of the model? (Recall that \mn serves as a \textit{proxy} for natural human object naming, since annotators were asked to name a highlighted object in an image isolated from its situative context; Section~\ref{sec:manynames}) Table~\ref{tab:exp_details_wrong} (and \ref{tab:exp_errors_agreement},  \ref{tab:exp_alternatives})
% \item \cs{Not examined at the moment -- MRR and Jaccard may be re-added for this: Are valid name alternatives among the top-N predictions?}
%\item do we get different evaluation results when testing existing object classification models on preferred responses from name distributions, as compared to naming responses collected from 1-3 workers (e.g., VisualGenome)? 
%\end{enumerate}

\subsection{Experimental Setup}
\label{sect:exp_setup}

\paragraph{Data}
\gbt{I moved this here cause I think it will be useful to define the different vocabularies here and then be able to simply refer to them later. Btw, consider using `MN-Entry' instead of `MN442'? but maybe having the number in the name is useful to be able to compare.}
\cs{todo after Section~\ref{sect:mn_overview} has been written.}
\begin{itemize}
	\item Test data: overlap of \mn and \vg test data (CITE)
	\item Target vocabulary of test data: overlap of MN442 and VG (\cs{XX} names)
	\item Source of label: \mn, but we compare against \vg
\end{itemize}

\paragraph{Models}

We test two object detection models trained on objects and names from the original \vg data, using the Faster R-CNN architecture \cite{fasterrcnn2015} which is initialized with features that were pre-trained on $1$K~ImageNet classes with the ResNet-101 classification model \cite{he2016deep}.
We use and retrain these two models with different target vocabularies:

\begin{small}
\begin{itemize}
\item FRCNN$_{\text{VG1600}}$: We use \citep{anderson2018updown}'s model\footnote{We used the code and model available at  \url{github.com/peteanderson80/bottom-up-attention}. The model is trained with an additional output over attributes, but we only use the object class prediction layer.} that is optimized for a set 1600 object names in \vg. This set was obtained by first taking the 2500 most frequent names in \vg and then (manually) filtering certain names.
 \item  FRCNN$_{\text{MN442}}$: we pre-process the \vg data as in \citeauthor{anderson2018updown}, but restrict the object classes to the $442$~entry-level names found in \mn (Section~\ref{sect:mn_analysis}). 
For training, we use a PyTorch reimplementation of Faster R-CNN\footnote{\url{github.com/jwyang/faster-rcnn.pytorch}}
\end{itemize}
\end{small}

The two former models are trained on \vg, which has almost 4 million labeled objects.
\mn is too small to train object detection models on it from scratch, which require learning millions of parameters.
On the other hand, obtaining enough naming data to be able to estimate entry-level names for larger datasets is very costly and probably unrealistic.
Therefore, we investigate the possibility of obtaining entry-level data via transfer learning, fine-tuning existing models with \mn data.
We do so with the standard methodology of taking the last hidden layer of existing Computer Vision models as input, and train object classification models to predict entry-level names (MN442).

We experiment with two kinds of (transferred) input features, different training vocabularies and ground-truth names for instances, similar to the detection models above.
This yields the following 5 transfer-learning-based naming models:

\begin{small}
\begin{itemize}
\item FRCNN$_{\text{VG1600}}$-VGMN:  This models extracts features from the object classification model FRCNN$_{\text{VG1600}}$\footnote{As our experimental results will show, FRCNN$_{\text{VG1600}}$ is more effective than FRCNN$_{\text{MN442}}$, so we only use the former model.}. We expect these to be very good features for our task, as they have been obtained with the same task (object classification) and dataset (\vg).  For each object (i.e.,~bounding box) in \mn, we extract its activation features from FRCNN's last hidden layer before the output layer.\cs{add size} The model uses the VGMN vocabulary (overlap of ManyNames and VG vocab).
\item ResNet-VGMN: For comparison, we consider a much less informed input, namely the features from the ResNet-101 object detection  pretrained on ImageNet \cite{+++}.
Results should of course be worse, and our aim is to assess the feasibility of using such generic object representations to obtain entry-level names. The model uses the VGMN vocabulary.  The model is optimized on VG and MN ground truth names respectively.
 \item  ResNet-MN442:  The same model as before, but trained with a vocab of 442 entry-level names. The model is optimized on VG and MN ground truth names respectively.
\end{itemize}
\end{small}


Note that all five models are optimized on the $25$K~objects in \mn. 
To predict the entry-level name of a target object~$o$, the classifier, a multi-layer neural network, is fed the object's features, and applies softmax activation to the output layer that corresponds to the target vocabulary. 

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{@{~}l@{~}|@{~}c@{~}|cc|cc@{~}}
		\toprule
		&  & \multicolumn{2}{c}{All} 
		& \multicolumn{2}{c}{VG$\neq$MN}\\
		&  & \multicolumn{2}{c}{($1072$)} 
		& \multicolumn{2}{c}{($223$)}\\	
		Model$_{\text{Vocab}}$ 
		&  GTtrain &  VG & MN 
		&  VG & MN \\ 
		\midrule
		FRCNN$_{\text{MN442}}$ & VG &  65.4 &      71.2 &   20.0 &      48.4  \\
		FRCNN$_{\text{VG1600}}$ & VG &    67.3 &      74.5 &    19.1 &      52.9  \\
		\midrule \midrule
		\multicolumn{6}{c}{Classifiers: Transfer learning on MN}\\
		Features$_{\text{Vocab}}$ &   \\
		\midrule 
		FRCNN$_{\text{VG1600}}$$_{\text{-VGMN}}$ & MN &    70.8 &      80.6 &    13.8 &      60.4  \\ 
		\midrule
		ResNet$_{\text{VGMN}}$ & MN  & 60.9 &  68.6 &  13.8 & 50.2  \\
		
		ResNet$_{\text{MN442}}$ & MN &            61.7 &              69.6 &                        13.8 &              50.7 \\
		ResNet$_{\text{VGMN}}$  &   VG &  62.4 &              62.9 &              28.4 &              31.1  \\
		%ResNet$_{\text{VGMN}}$\_4ep  &   VG &  62.4 &              62.9 &              28.4 &              31.1  \\
		%ResNet101$_{\text{VGMN}}$\_8ep & VG &            63.9 &              62.6 &          34.2 &              28.0 \\
		ResNet$_{\text{MN442}}$ & VG  &            63.7 &              63.7 &                  30.7 &              31.1  \\
		\bottomrule
	\end{tabular}
	\caption{Model accuracy (in \%) on the \mn test objects. Vocab denotes the dataset used to induce the target vocabulary for training (the numbers give the size of the vocabulary). GTtrain gives the dataset that provides the ground truth labels during training, and VG and MN for testing \label{tab:exp_VGvsMN}.}
\end{table}


%\paragraph{Measures}


\subsection{Results}
\label{sect:exp_results}

\gbt{To do: streamline to make consistent with motivations + expectations laid out in the previous subsection.}

Table \ref{tab:exp_VGvsMN} shows the accuracy of all models on the \mn test data. 
We compare model effectiveness on predicting the entry-level name (columns MN) against predicting the \arbitrary VG name (columns VG), and furthermore report results on the test instances where the two names differ (block VG$\neq$MN) \gbt{add motivation for the latter, and make sure we address this when we discuss results; else, remove.}. 
\begin{itemize}
	\item The FRCNN models \gbt{all models, actually} are better on predicting the entry-level name (columns MN) than the \arbitrary VG name (columns VG). 
	This shows that they do learn entry-level naming to some extent, despite being trained on VG, i.e.,~\arbitrary ground truth labels.
	\cs{I would not comment on diff. btw. the two FRCNNs, it's small compared to their diffs to the classifiers, and may be also due to the diff. implementations, and VG1600 being trained additionally on attributes.}
	\item The overall best model is FRCNN$_\text{VG1600}$$_\text{-VGMN}$, the Vanilla classifier that uses  FRCNN$_\text{VG1600}$ features and is trained on the \mn training data. 
	\item \cs{?What to keep here:} The classifiers using ResNet features are the least effective, but come close to the FRCNN object detectors on entry-level naming (columns MN, GTtrain MN), or even better  (columns VG, GTtrain VG) despite being trained on a fraction of FRCNN's training size using a simple Vanilla classifier in contrast to a fully-fledged CNN architecture.
	This confirms what we observed on the FRCNN results: and follow the pattern of learning entry-level naming  
	\item Models learn entry-level naming when being trained on \arbitrary naming data, obtaining better results on entry-level prediction than \arbitrary name prediction for the same instances, and benefit from being trained on entry-levels using transfer learning. \cs{overfitting?}
	\item Comparing the target vocabulary, MN442 and VG1600/VGMN, we see that the restriction on a smaller, but empirically derived vocabulary of entry-level names only, is not beneficial, with all MN442 models being worse or comparable to VGMN/VG100.
\end{itemize}


\subsection{Discussion}
\label{sect:exp_discussion}

%\iffalse
\cs{under construction}
\begin{itemize}
	\item Object detection models in CV have different goal than L+V object recognition models (see related work--former is on labels, latter on predicting natural language). 
	However, the former, pre-trained towards labels, are the backbone / used as feature extractors for the latter.
	\item ... 
	\item As we explained, there is a high variation of object names, and objects may be named by multiple alternative names (\cs{vocab size MN vs. vocab size VG for same object set}). 
	Yet, humans usually have a clearly preferred name for individual object instances (and the set of those is relatively small)--humans agree on a particular name for an instance (entry-level name).
	\item Hence, to model human object naming, datasets with many/dozens of annotations for the same instance are required. 
	\item We argue that such datasets are more reliable in that they provide empirically derived preferred names (entry-level names), and richer--the provide valid name alternatives. 
	\item But since their elicitation is expensive and time-consuming, it is not realistic to create training datasets of dozens of object names for pre-training features with CV models, which need a huge amount of training data. 
\end{itemize}
%\fi

\iffalse
We propose to use datasets of object names for evaluating models on the task of object naming (depending on results: also valuable for comparing object detection/image classification models).
Specifically, we analyse object detection [and classification] models on the task of human object naming, using the \mn dataset as test data:\\
Can object detection models, trained on arbitrary names, account for human object naming?
\begin{itemize}
	\item Do they predict the entry-level name?
	\item Are valid name alternatives among the top-N predictions?
x	\item Do models make similar mistakes as humans when being faced with the task of naming highlighted objects in images [i.e.,\ the artificial setup of object naming for data annotation]?\\
	-- naming an alternative object (maybe more salient)\\
	-- predicting a semantically related name
	%do we get different evaluation results when testing existing object classification models on preferred responses from name distributions, as compared to naming responses collected from 1-3 workers (e.g., VisualGenome)? 
	\item Can we use \mn as fine-tuning dataset? (Here: only Vanilla model)\\
	-- can we learn entry-level names by fine-tuning object detection models on \mn?\\
	-- Can we directly learn entry-level names by fine-tuning image classification models on \mn, i.e.,\ the pre-trained features to initialize object detection models?
\end{itemize}
\fi

\iffalse
OLD TABLE WITH MORE MEASURES

\begin{table*}[t]
	\centering
	\small
	\begin{tabular}{l|c|r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r|@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}}
		\toprule
		&   & \multicolumn{6}{c}{All Test Images ($\#$)} 
		& \multicolumn{6}{c}{VG$\neq$MN Images ($\#$)}\\	
		Model$_{\text{Vocab}}$	 
		&  GTtrain &  =VG & =MN & $\in$MN  & KL & J & MRR & AvgMRR 
		&  =VG & =MN & $\in$MN  & KL & J & MRR & AvgMRR\\ 
		\midrule
		FRCNN--MN442 & VG &            65.4 &              71.2 &                85.6 &         1.0 &             69.8 &          0.8 &             0.7 &            20.0 &              48.4 &                78.7 &         1.4 &             60.4 &          0.7 &             0.5 \\
		FRCNN--VG2500 & VG \\
		FRCNN$_{\text{VG1600}}$ & VG &            67.3 &              74.5 &                89.2 &         0.6 &             74.3 &          0.8 &             0.7 &            19.1 &              52.9 &                86.2 &         0.8 &             69.4 &          0.7 &             0.6 \\
		\midrule \midrule
		& \multicolumn{12}{c}{Classifiers: Fine-tuning pre-trained image features on \mn}\\
		Features$_{\text{Vocab}}$ & GTtrain  \\
		\midrule 
		FRCNN$_{\text{VG1600}}$$_{\text{VGMN}}$ & MN &            70.8 &              80.6 &                90.1 &         4.7 &             62.0 &          0.8 &             0.6 &            13.8 &              60.4 &                85.8 &         4.6 &             47.3 &          0.7 &             0.5 \\ 
		FRCNN$_{\text{VG1600}}$--MN442 &  MN \\
		\midrule
		ResNet101$_{\text{VGMN}}$ & MN  &            60.9 &              68.6 &                77.9 &         4.9 &             56.4 &          0.8 &             0.6 &            13.8 &              50.2 &                73.3 &         4.7 &             42.9 &          0.6 &             0.4 \\
		
		ResNet101--MN442 & MN &            61.7 &              69.6 &                78.9 &         4.9 &             57.4 &          0.8 &             0.6 &            13.8 &              50.7 &                73.8 &         4.6 &             45.1 &          0.6 &             0.5 \\
		ResNet101$_{\text{VGMN}}$\_4ep  &   VG &  62.4 &              62.9 &                75.0 &         5.0 &             53.7 &          0.7 &             0.6 &            28.4 &              31.1 &                64.9 &         4.8 &             39.1 &          0.4 &             0.4 \\
		ResNet101$_{\text{VGMN}}$\_8ep & VG &            63.9 &              62.6 &                75.6 &         5.0 &             53.9 &          0.7 &             0.6 &            34.2 &              28.0 &                66.2 &         4.9 &             39.7 &          0.4 &             0.4 \\
		ResNet101--MN442 & VG  &            63.7 &              63.7 &                76.6 &         5.0 &             55.4 &          0.7 &             0.6 &            30.7 &              31.1 &                67.1 &         4.8 &             41.0 &          0.4 &             0.4 \\
		\bottomrule
	\end{tabular}
	\caption{Target vocabulary in test data: MN442. Vocab denotes the dataset from which the target vocabulary for training was induced (the numbers give the size of the vocabulary). GTtrain denotes the dataset from which the ground truth labels are obtained during \textit{training}. MRR is mean reciprocal rank; J is Jaccard score. Note that we considered all name responses in MN, including those with $\text{count}(name)<2$\label{tab:entrylevels}. }
\end{table*}


\begin{table*}[t]
	\centering
	\small
	\begin{tabular}{l@{~}|rrrr}
		\toprule
		... \\
		\midrule
		Domain	 & ... \\ 
		\midrule
		All           \\
		home           \\
		food           \\
		buildings      \\
		vehicles       \\
		animals\_plants \\
		clothing       \\
		people         \\
		\bottomrule
	\end{tabular}
	\caption{RESULTS FOR SELECTED MODELS \label{tab:domains_bestmodel}}
\end{table*}

% \subsection{[TBC] Generalization Ability: OpenImages}
% \label{sect:exp_openimages}
% Question: Can models trained towards \mn generalize to other, related datasets? Here: OpenImages. \cs{[Increase coverage with zero-shot learning?]}\

% Models compared: FRCNN$_{\text{VG1600}}$ vs. FRCNN$_{\text{MN442}}$ vs. ResNet101--XX (best Vanilla).
\fi


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "acl2020_main"
%%% End:
