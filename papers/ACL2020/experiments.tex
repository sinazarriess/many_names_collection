% experiments.tex

Given the entry-level naming data from \mn, which extends the more \arbitrary naming data from VisualGenome (VG), we now have a variety of possibilities to train, re-train or fine-tune existing object detection or classification models, to capture natural naming.
Moreover, we can systematically vary the vocabulary for these models during training: a standard object naming vocabulary compiled from a frequency list as in \cite{anderson2018updown}, or a set of entry-level names as aggregated from the ManyNames data.
Our experiments will assess these various modeling decisions for the task of entry-level naming, in the current section by using accuracy as a standard evaluation metric.
Section~\ref{sec:analysis} will analyze the results with respect to the different types of errors identified by our verification data.
% found in the verification data in Section \ref{sec:verification}.
%We leave modeling experiments that take into account other possible names of objects for future work.

% Our goal is to experimentally obtain insights into the usefulness of computer vision object labeling models, that are commonly used for \lv methods, to account for natural human object naming. 
% Specifically, our research questions are
% \begin{enumerate}
% 	\item Do object detection models, despite being trained on \arbitrary  
% 	names, account for human object naming?\cs{repetitive}
% 	\item Can we apply simple transfer learning on \mn to train classifiers towards this naming task by utilizing pre-trained object detection or image classification model as feature extractors? Figure~\ref{fig:exp_confusions} \\
% 	\cs{move to models: -- can we learn entry-level names by fine-tuning object detection models on \mn?\\
% 		-- Can we directly learn entry-level names by fine-tuning image classification models on \mn, i.e.,\ the pre-trained features to initialize object detection models?}
% \end{enumerate}
% TO BE COVERED BY THE SUBSECTIONS BELOW:
% \begin{enumerate}
% 	\item Do models prefer the entry-level name of an object over an \arbitrary name? Table~\ref{tab:exp_VGvsMN}
% 	\item In cases of failure, do they predict alternative, but less preferred  names instead? Tables~\ref{tab:exp_overview_results}, \ref{tab:exp_alternatives}
% 	\item What phenomena of human behavior on the object naming task of \mn do the models mirror? That is, which model mistakes or confusions are due to the artificial setup of the task itself, and less a failure of the model? (Recall that \mn serves as a \textit{proxy} for natural human object naming, since annotators were asked to name a highlighted object in an image isolated from its situative context; Section~\ref{sec:manynames}) Table~\ref{tab:exp_details_wrong} (and \ref{tab:exp_errors_agreement},  \ref{tab:exp_alternatives})
% \item \cs{Not examined at the moment -- MRR and Jaccard may be re-added for this: Are valid name alternatives among the top-N predictions?}
%\item do we get different evaluation results when testing existing object classification models on preferred responses from name distributions, as compared to naming responses collected from 1-3 workers (e.g., VisualGenome)? 
%\end{enumerate}

\subsection{Experimental Setup}
\label{sect:exp_setup}

\paragraph{Data}
We split \mn into training ($21K$~objects) and test data ($1,072$), where the test data corresponds to the overlap of \mn (MN) and the \vg (VG) test split used by  \newcite{anderson2018updown}. 
The target vocabulary of the test data is MN442: the 442 entry-level names we derived from \mn. %the overlap of MN442 and VG ($355$ names). 
For transfer learning we train models on the ground truth entry-level names provided by \mn, and compare this to training on \vg labels. 

\paragraph{Models}
We test two object detection models trained on objects and names from the original \vg data, using the Faster R-CNN architecture \cite{fasterrcnn2015}, initialized with features that were pre-trained on $1$K~ImageNet classes with the ResNet-101 classification model \cite{he2016deep}.
We use and retrain these two models with different target vocabularies:

\begin{itemize}
\item FRCNN$_{\text{VG1600}}$: This is \citep{anderson2018updown}'s model\footnote{We used the code and model available at  \url{github.com/peteanderson80/bottom-up-attention}. The model is trained with an additional output over attributes, but we only use the object class prediction layer.} that is optimized for a set of 1600 object names in \vg. This set was obtained by first taking the 2500 most frequent names in \vg and then (manually) filtering certain names.
 \item  FRCNN$_{\text{MN442}}$: we pre-process the \vg data as in \citeauthor{anderson2018updown}, but restrict the object classes to the $442$~entry-level names found in \mn (Section~\ref{sect:mn_analysis}). 
For training, we use a PyTorch reimplementation of Faster. R-CNN\footnote{\url{github.com/jwyang/faster-rcnn.pytorch}}
\end{itemize}

The two former models are trained on \vg, which has almost 4 million labeled objects.
\mn is too small to train object detection models on it from scratch, which requires learning millions of parameters.
On the other hand, obtaining enough naming data to be able to estimate entry-level names for larger datasets is very costly and probably unrealistic.
Therefore, we investigate the possibility of obtaining effective entry-level naming models via transfer learning, fine-tuning existing models with \mn data.
We do so with the standard methodology of taking the last hidden layer of existing Computer Vision models as input, and train object classification models to predict entry-level names (MN442).

We experiment with two kinds of input features, as well as different training vocabularies and ground-truth names for instances, similar to the detection models above.
This yields the following 4 transfer-learning-based naming models:
\begin{itemize}
\item FRCNN$_{\text{VG1600--VGMN}}$:
	This model extracts features from the object classification model FRCNN$_{\text{VG1600}}$\footnote{
	As experimental results will show, FRCNN$_{\text{VG1600}}$ is more effective than FRCNN$_{\text{MN442}}$, so we only use the former.
}. We expect these to be very good features for our task, as they have been obtained with the same task (object classification) and dataset (\vg).  For each object (i.e.,~bounding box) in \mn, we extract its activation features from FRCNN's last hidden layer before the output layer. The model uses the VGMN vocabulary (overlap of the full ManyNames vocabulary with VG).
  \item ResNet-VGMN: 
For comparison, we consider a much less informed input, namely the features from the ResNet-101 image classification model, that was used to initialize the FRCNN models above, and pretrained on ImageNet \cite{imagenet_cvpr09}. 
Results should of course be worse, and our aim is to assess the feasibility of using such generic object representations to obtain entry-level names. ResNet-VGMN uses the VGMN vocabulary; we trained two models, optimized on VG and MN ground truth names, respectively, but report only the one optized on VG (the other one yielded similar output).
 \item  ResNet-MN442 (two models):
 The same model as ResNet-VGMN, but trained with a vocabulary of 442 entry-level names; we optimized for on VG or MN ground truth names respectively.
\end{itemize}
Note that these four models are optimized on the $21$K~objects in \mn. 
To predict the entry-level name of a target object~$o$, the classifier, a multi-layer neural network, is fed the object's features, and applies softmax activation to the output layer that corresponds to the target vocabulary. 

%\iffalse
%\begin{table}[t]
%	\centering
%	\small
%	\begin{tabular}{@{~}l@{~}|@{~}c@{~}|cc|cc@{~}}
%		\toprule
%		&  & \multicolumn{2}{c}{All} 
%		& \multicolumn{2}{c}{VG$\neq$MN}\\
%		&  & \multicolumn{2}{c}{($1072$)} 
%		& \multicolumn{2}{c}{($223$)}\\	
%		Model$_{\text{Vocab}}$ 
%		&  GTtrain &  VG & MN 
%		&  VG & MN \\ 
%		\midrule
%		FRCNN$_{\text{MN442}}$ & VG &  65.4 &      71.2 &   20.0 &      48.4  \\
%		FRCNN$_{\text{VG1600}}$ & VG &    67.3 &      74.5 &    19.1 &      52.9  \\
%		\midrule \midrule
%		\multicolumn{6}{c}{Classifiers: Transfer learning on MN}\\
%		Features$_{\text{Vocab}}$ &   \\
%		\midrule 
%		FRCNN$_{\text{VG1600}}$$_{\text{-VGMN}}$ & MN &    70.8 &      80.6 &    13.8 &      60.4  \\ 
%		\midrule
%		ResNet$_{\text{VGMN}}$ & MN  & 60.9 &  68.6 &  13.8 & 50.2  \\
%		
%		ResNet$_{\text{MN442}}$ & MN &            61.7 &              69.6 &                        13.8 &              50.7 \\
%		ResNet$_{\text{VGMN}}$  &   VG &  62.4 &              62.9 &              28.4 &              31.1  \\
%		%ResNet$_{\text{VGMN}}$\_4ep  &   VG &  62.4 &              62.9 &              28.4 &              31.1  \\
%		%ResNet101$_{\text{VGMN}}$\_8ep & VG &            63.9 &              62.6 &          34.2 &              28.0 \\
%		ResNet$_{\text{MN442}}$ & VG  &            63.7 &              63.7 &                  30.7 &              31.1  \\
%		\bottomrule
%	\end{tabular}
%	\caption{Model accuracy (in \%) on the \mn test objects. Vocab denotes the dataset used to induce the target vocabulary for training (the numbers give the size of the vocabulary). GTtrain gives the dataset that provides the ground truth labels during training, and VG and MN for testing \label{tab:exp_VGvsMN}.}
%\end{table}
%\fi
%\paragraph{Measures}


\subsection{Results}
\label{sect:exp_results}

Table \ref{tab:exp_VGvsMN} shows the accuracy of the models on the \mn test data.
%\footnote{ResNet$_{\text{VGMN}}$ with MN is comparable to its counterpart.}. 
We compare model effectiveness on predicting the entry-level name (column MN) against predicting the VG name . 
%We therefore consider an object's VG name less likely to represent the entry-level name than its MN name. 

% and furthermore report results on the test instances where the two names differ (block VG$\neq$MN). 
%Note that the VG names were derived on the basis of a few annotations per object only, and, as shown in Section~\ref{sect:mn_analysis}, even ``just'' $10$ or $20$ names sufficiently likely yield an entry-level name for only $68\%$ or $78\%$ instances, respectively.  
%We therefore consider an object's VG name less likely to represent the entry-level name than its MN name. 
% \gbt{add motivation for the latter, and make sure we address this when we discuss results; else, remove.}. 
%\cs{Left as items for the sake of a better overview for now}
\begin{table}[t]
	\centering
	\small
	\begin{tabular}{@{~}l@{~}|@{~}c@{~}|cc@{~}}
		\toprule
		&  & \multicolumn{2}{c}{GTtest} \\
		Model$_{\text{Vocab}}$ &  GTtrain &  VG & MN  \\ 
		\midrule
		FRCNN$_{\text{MN442}}$ & VG &  65.4 &      71.2   \\
		FRCNN$_{\text{VG1600}}$ & VG &    67.3 &      {\bf 74.5} \\
		\midrule \midrule
		\multicolumn{4}{c}{Classifiers: Transfer learning on MN}\\
		Features$_{\text{Vocab}}$ &   \\
		\midrule 
		FRCNN$_{\text{VG1600}}$$_{\text{-VGMN}}$ & MN &    {\bf 70.8} &      {\bf 80.6}  \\ 
		\midrule
		%ResNet$_{\text{VGMN}}$ & MN  & 60.9 &  68.6   \\
		
		ResNet$_{\text{MN442}}$ & MN &            61.7 &              69.6  \\
		ResNet$_{\text{VGMN}}$  &   VG &  62.4 &              62.9  \\
		%ResNet$_{\text{VGMN}}$\_4ep  &   VG &  62.4 &              62.9 &              28.4 &              31.1  \\
		%ResNet101$_{\text{VGMN}}$\_8ep & VG &            63.9 &              62.6 &          34.2 &              28.0 \\
		ResNet$_{\text{MN442}}$ & VG  &            63.7 &              63.7   \\
		\bottomrule
	\end{tabular}
	\caption{Model accuracy (in \%) on the \mn test objects. Vocab denotes the dataset used to induce the target vocabulary for training (the numbers are vocabulary size). GTtrain gives the dataset providing the ground truth labels for training, and GTtest for testing \label{tab:exp_VGvsMN}.}
	\vspace{0ex}
\end{table}

A pervasive trend is that all models perform better 
at predicting the entry-level name (MN) than the VG name (VG), regardless of which vocabulary (Vocab) and which ground-truth (GTtrain) they are trained on. 
Note that the VG names were derived on the basis of 1-2 annotations per object, which, as shown in Section~\ref{sect:mn_analysis}, is not enough to robustly estimate entry-level names.
%even ``just'' $10$ or $20$ names sufficiently likely yield an entry-level name for only $68\%$ or $78\%$ instances, respectively. 
The results therefore indicate that, even when not trained to do so (FRCNN$_{\text{VG1600}}$), models tend to predict entry-level names.
This is likely because entry-level names better reflect generalizations across objects.
% the FRCNN detection models do learn entry-level naming, with an accuracy of up to~$74.5$\%.

The overall best model is FRCNN$_\text{VG1600}$$_\text{-VGMN}$ (the transfer learning classifier with FRCNN$_\text{VG1600}$ features, trained on the \mn).
Fine-tuning on entry-level names also helps in the original task of predicting VG names, as FRCNN$_\text{VG1600}$$_\text{-VGMN}$ also improves in the VG evaluation (70.8\% accuracy, vs.\ \ 67.3\% without fine-tuning).
As expected, the classifiers using ResNet features obtain the worst results, but are still remarkably effective considering that they were trained on a fraction of FRCNN's training size using a simple transfer learning classifier.
%, in contrast to the fully-fledged CNN architecture of the FRCNN object detectors, the drop in effectiveness on entry-level naming (column MN, GTtrain MN) is decently low. 
%This confirms what we observed on the FRCNN results. 

In sum, we find that models learn entry-level naming even when being trained on \arbitrary naming data, obtaining better results on entry-level prediction than VG name prediction for the same instances, and that they benefit from transfer learning with entry-level names. Instead, restricting the training vocabulary to entry-level names is not effective (all MN422 models are worse or comparable to VGMN/VG1600 models).
% Comparing the target vocabulary, MN442 and VG1600/VGMN, we see that the restriction on a smaller, but empirically derived vocabulary of entry-level names only, is not beneficial, with all MN442 models being worse or comparable to VGMN/VG100.


\subsection{Discussion}
\label{sect:exp_discussion}

%To date, little research has been done on how to finetune and calibrate the \textit{vocabulary} of object detection and classification models from Computer Vision, for purposes of \lv.
As Section~\ref{sec:manynames} showed, object naming is a linguistically complex phenomenon which exhibits a fair amount of agreement and also variation and potential errors.
Hence, it is striking that even models that are trained on \arbitrary names capture entry-level naming to a good extent.
However, our results also suggest that current \textit{evaluation} methods might not be very reliable when applied on \arbitrary names, probably punishing models for `errors' that actually constitute plausible alternatives.
The next section elaborates on this and proposes a fairer assessment of naming quality.

 
%\iffalse
%\cs{under construction}
%\begin{itemize}
%	\item Object detection models in CV have different goal than L+V object recognition models (see related work--former is on labels, latter on predicting natural language). 
%	However, the former, pre-trained towards labels, are the backbone / used as feature extractors for the latter.
%	\item ... 
%	\item As we explained, there is a high variation of object names, and objects may be named by multiple alternative names (\cs{vocab size MN vs. vocab size VG for same object set}). 
%	Yet, humans usually have a clearly preferred name for individual object instances (and the set of those is relatively small)--humans agree on a particular name for an instance (entry-level name).
%	\item Hence, to model human object naming, datasets with many/dozens of annotations for the same instance are required. 
%	\item We argue that such datasets are more reliable in that they provide empirically derived preferred names (entry-level names), and richer--the provide valid name alternatives. 
%	\item But since their elicitation is expensive and time-consuming, it is not realistic to create training datasets of dozens of object names for pre-training features with CV models, which need a huge amount of training data. 
%\end{itemize}
%\fi

%\iffalse
%We propose to use datasets of object names for evaluating models on the task of object naming (depending on results: also valuable for comparing object detection/image classification models).
%Specifically, we analyse object detection [and classification] models on the task of human object naming, using the \mn dataset as test data:\\
%Can object detection models, trained on arbitrary names, account for human object naming?
%\begin{itemize}
%	\item Do they predict the entry-level name?
%	\item Are valid name alternatives among the top-N predictions?
%x	\item Do models make similar mistakes as humans when being faced with the task of naming highlighted objects in images [i.e.,\ the artificial setup of object naming for data annotation]?\\
%	-- naming an alternative object (maybe more salient)\\
%	-- predicting a semantically related name
%	%do we get different evaluation results when testing existing object classification models on preferred responses from name distributions, as compared to naming responses collected from 1-3 workers (e.g., VisualGenome)? 
%	\item Can we use \mn as fine-tuning dataset? (Here: only Vanilla model)\\
%	-- can we learn entry-level names by fine-tuning object detection models on \mn?\\
%	-- Can we directly learn entry-level names by fine-tuning image classification models on \mn, i.e.,\ the pre-trained features to initialize object detection models?
%\end{itemize}
%\fi

%\iffalse
%OLD TABLE WITH MORE MEASURES
%
%\begin{table*}[t]
%	\centering
%	\small
%	\begin{tabular}{l|c|r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r|@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}}
%		\toprule
%		&   & \multicolumn{6}{c}{All Test Images ($\#$)} 
%		& \multicolumn{6}{c}{VG$\neq$MN Images ($\#$)}\\	
%		Model$_{\text{Vocab}}$	 
%		&  GTtrain &  =VG & =MN & $\in$MN  & KL & J & MRR & AvgMRR 
%		&  =VG & =MN & $\in$MN  & KL & J & MRR & AvgMRR\\ 
%		\midrule
%		FRCNN--MN442 & VG &            65.4 &              71.2 &                85.6 &         1.0 &             69.8 &          0.8 &             0.7 &            20.0 &              48.4 &                78.7 &         1.4 &             60.4 &          0.7 &             0.5 \\
%		FRCNN--VG2500 & VG \\
%		FRCNN$_{\text{VG1600}}$ & VG &            67.3 &              74.5 &                89.2 &         0.6 &             74.3 &          0.8 &             0.7 &            19.1 &              52.9 &                86.2 &         0.8 &             69.4 &          0.7 &             0.6 \\
%		\midrule \midrule
%		& \multicolumn{12}{c}{Classifiers: Fine-tuning pre-trained image features on \mn}\\
%		Features$_{\text{Vocab}}$ & GTtrain  \\
%		\midrule 
%		FRCNN$_{\text{VG1600}}$$_{\text{VGMN}}$ & MN &            70.8 &              80.6 &                90.1 &         4.7 &             62.0 &          0.8 &             0.6 &            13.8 &              60.4 &                85.8 &         4.6 &             47.3 &          0.7 &             0.5 \\ 
%		FRCNN$_{\text{VG1600}}$--MN442 &  MN \\
%		\midrule
%		ResNet101$_{\text{VGMN}}$ & MN  &            60.9 &              68.6 &                77.9 &         4.9 &             56.4 &          0.8 &             0.6 &            13.8 &              50.2 &                73.3 &         4.7 &             42.9 &          0.6 &             0.4 \\
%		
%		ResNet101--MN442 & MN &            61.7 &              69.6 &                78.9 &         4.9 &             57.4 &          0.8 &             0.6 &            13.8 &              50.7 &                73.8 &         4.6 &             45.1 &          0.6 &             0.5 \\
%		ResNet101$_{\text{VGMN}}$\_4ep  &   VG &  62.4 &              62.9 &                75.0 &         5.0 &             53.7 &          0.7 &             0.6 &            28.4 &              31.1 &                64.9 &         4.8 &             39.1 &          0.4 &             0.4 \\
%		ResNet101$_{\text{VGMN}}$\_8ep & VG &            63.9 &              62.6 &                75.6 &         5.0 &             53.9 &          0.7 &             0.6 &            34.2 &              28.0 &                66.2 &         4.9 &             39.7 &          0.4 &             0.4 \\
%		ResNet101--MN442 & VG  &            63.7 &              63.7 &                76.6 &         5.0 &             55.4 &          0.7 &             0.6 &            30.7 &              31.1 &                67.1 &         4.8 &             41.0 &          0.4 &             0.4 \\
%		\bottomrule
%	\end{tabular}
%	\caption{Target vocabulary in test data: MN442. Vocab denotes the dataset from which the target vocabulary for training was induced (the numbers give the size of the vocabulary). GTtrain denotes the dataset from which the ground truth labels are obtained during \textit{training}. MRR is mean reciprocal rank; J is Jaccard score. Note that we considered all name responses in MN, including those with $\text{count}(name)<2$\label{tab:entrylevels}. }
%\end{table*}
%
%
%\begin{table*}[t]
%	\centering
%	\small
%	\begin{tabular}{l@{~}|rrrr}
%		\toprule
%		... \\
%		\midrule
%		Domain	 & ... \\ 
%		\midrule
%		All           \\
%		home           \\
%		food           \\
%		buildings      \\
%		vehicles       \\
%		animals\_plants \\
%		clothing       \\
%		people         \\
%		\bottomrule
%	\end{tabular}
%	\caption{RESULTS FOR SELECTED MODELS \label{tab:domains_bestmodel}}
%\end{table*}
%
%% \subsection{[TBC] Generalization Ability: OpenImages}
%% \label{sect:exp_openimages}
%% Question: Can models trained towards \mn generalize to other, related datasets? Here: OpenImages. \cs{[Increase coverage with zero-shot learning?]}\
%
%% Models compared: FRCNN$_{\text{VG1600}}$ vs. FRCNN$_{\text{MN442}}$ vs. ResNet101--XX (best Vanilla).
%\fi


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "acl2020_main"
%%% End:
