Generally, research in Language \& Vision (L\&V) is interested in modeling how speakers naturally talk about visual objects and scenes, in contrast to the fixed image labeling schemes used in Computer Vision.
Data collections in L\&V are typically set up as free annotation tasks,  where subjects are free to produce whatever word or utterance they consider most suitable for the given task (e.g.\ image description, reference to objects, visual dialogue), which naturally results in linguistic variation.
For this reason, large-scale data collections in L\&V usually provide a certain amount of parallel annotations for the same entity from different annotators \cite{fangetal:2015,devlin:imcaqui,Kazemzadeh2014,mao15,vries2017guesswhat}.

Compared to work on perception and language grounding in Linguistics or Cognitive Science, however, recent data collections in L\&V capture a rather limited amount of inter-speaker variation.
For instance, so-called picture naming norms used in Psycholinguistics record naming responses for hundreds of speakers for the same object  \cite{snodgrass,rossion2004revisiting}, whereas captioning or referring expression data sets typically provide less than 10 annotations per entity \cite{devlin:imcaqui,Kazemzadeh2014,mao15}.
Consequently, a reliable assessment of inter-annotator agreement, speaker preferences, and a deeper linguistic analysis of the observed variation is mostly not possible with available data collections in  L\&V.
At the same time, these datasets have much potential for research on these topics, as they provide more realistic images of real-world objects and scenes than the idealized drawings used in picture naming norms (see Figure \ref{fig:cake}), as well as a wider coverage of object categories.
A more systematic understanding of the factors influencing the choice of object names could inform theoretical work on language grounding and pragmatics \cite{rohde2012communicating,graf2016animal}, as well as the design of models and architectures in L\&V \cite{lazaridou-dinu-baroni:2015:ACL-IJCNLP,Ordonez:2016,zhao2017open}.

In this paper, we present ManyNames, a dataset with substantial amounts of object names per object for real-world images.
We start by surveying existing resources in L\&V that provide object names: \refcoco \cite{Yu2016}, a collection of referring expressions, \flickr \cite{plummer2015flickr30kentities}, which provides region-to-phrase linkings for Flickr 30K captions \cite{young:2014}, and \vgenome \cite{krishna2016visualgenome}, which features extensive region-level annotations. We highlight their potential contributions to the study of object naming, and also argue that the low number of annotations per item prevents reliable assessment or linguistic analysis of object-specific preferences and naming variation.

\begin{figure}[tbp]
\scriptsize
\begin{tabular}{p{4.3cm}p{2cm}}
%VisualGenome+ManyNames & \cite{snodgrass}\\
\centering
\includegraphics[scale=0.15]{figures/2390077_1254219_supercat_unique.png} &
\includegraphics[scale=0.4]{figures/snodgrass_vanderwart_cake_042.png}\\
 cake\ (53),  food\ (19), bread\ (8), burger\ (6), dessert\ (6), snacks\ (3), muffin\ (3),  pastry\ (3) & \hspace{.9cm} cake (83)
\end{tabular}
\caption{Names for a cake object in ManyNames (left) and in Snodgrass's Naming Norms (right), percentages of responses in parentheses.}
\label{fig:cake}
\end{figure}

To address this shortcoming, we contribute a new dataset, ManyNames, that contains 36 crowd-sourced names for 25K instances from \vgenome.%
\footnote{The dataset will be available upon publication.}
The number of annotations per object available in ManyNames is considerably larger than in recent data collections in L\&V.
%Moreover, our images show objects in complex visual contexts,
%unlike the ``clean'' ImageNet data~\cite{imagenet_cvpr09} that is popular in Computer Vision \cite{ILSVRC15}, and %unlike stylized line drawings used in picture naming experiments in Cognitive Science (Figure \ref{fig:cake}).

The trends we identify in the dataset are illustrated in Figure \ref{fig:cake} (left): Our data reveals clear naming preferences (in the example, 53\% of the annotators prefer the so-called basic-level name \textit{cake}, see Section \ref{subsec:rosch} for further explananation) and also rich variation (the remaining annotators prefer other options like \textit{food, dessert, bread}) that is not restricted to taxonomic relations studied in previous work on naming \cite{Ordonez:2016,graf2016animal}: while \word{food} is in a taxonomic relation to \word{cake} (it is a hypernym), \textit{dessert} highlights a different facet of the object.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "lrec2020naming"
%%% End:
