Generally, research in Language \& Vision (L\&V) is interested in modeling how speakers \textit{naturally} talk about visual objects and scenes, in contrast to the fixed image labeling schemes used in Computer Vision.
Data collections in L\&V are typically set up as free annotation tasks,  where annotators (or speakers) are free to produce whatever word or utterance they consider most suitable for the given task (e.g.\ image description, reference to objects, visual dialogue) -- naturally results in linguistic variation.
For this reason, large-scale data collections in L\&V usually provide a certain amount of parallel annotations for the same entity from different annotators \cite{fangetal:2015,devlin:imcaqui,Kazemzadeh2014,mao15,vries2017guesswhat}.

As compared to work on perception and language grounding in Linguistics or Cognitive Science, however, recent data collections in L\&V capture a rather limited amount of inter-speaker variation.
For instance, so-called picture naming norms used in Psycholinguistics typically record naming responses for hundreds of speakers for the same object  \cite{snodgrass,rossion2004revisiting}, whereas captioning or referring expression data sets typically provide less than 10 annotations per entity \cite{devlin:imcaqui,Kazemzadeh2014,mao15}.
Consequently, a reliable assessment of inter-annotator agreement, speaker preferences and a deeper linguistic analysis of the observed variation is mostly out of scope with available data collections in  L\&V.
At the same time, these datasets provide more realistic images of real-world objects and scenes, and a wider coverage of object categories, compared to the idealized drawings used in picture naming norms, as shown in Figure \ref{fig:cake}.

In this paper, we take a first step towards building a dataset with object naming data for real-world images. We believe that a more systematic understanding of the factors underlying variation in object naming could inform theoretical work on language grounding and pragmatics \cite{rohde2012communicating,graf2016animal}, as well as the design of models and architectures in L\&V that deal with a large and natural vocabulary \cite{lazaridou-dinu-baroni:2015:ACL-IJCNLP,Ordonez:2016,zhao2017open}. As a starting point, we compare three popular datasets in L\&V: \refcoco \cite{Yu2016}, a collection of referring expressions, \flickr \cite{plummer2015flickr30kentities}, which provides region-to-phrase linkings for Flickr 30K captions \cite{young:2014}, and \vgenome \cite{krishna2016visualgenome}, which features extensive region-level annotations. We observe that the object name annotations in these corpora suggest that there is a certain amount of variation, but the low number of annotations per item does not allow for any reliable assessment or linguistic analysis.

\begin{figure}[tbp]
\scriptsize
\begin{tabular}{p{4.3cm}p{2cm}}
%VisualGenome+ManyNames & \cite{snodgrass}\\
\centering
\includegraphics[scale=0.15]{figures/2390077_1254219_supercat_unique.png} &
\includegraphics[scale=0.4]{figures/snodgrass_vanderwart_cake_042.png}\\
 cake\ (53),  food\ (19), bread\ (8), burger\ (6), dessert\ (6), snacks\ (3), muffin\ (3),  pastry\ (3) & \hspace{.9cm} cake (83)
\end{tabular}
\caption{Names for a cake object in ManyNames (left) and in Snodgrass's Naming Norms (right), percentages of responses in parentheses.}
\label{fig:cake}
\end{figure}

To address this shortcoming, we contribute a new dataset, ManyNames, that contains 36 crowd-sourced names for 25K instances from \vgenome.%
\footnote{The dataset will be available upon publication.}
The number of annotations per object available in ManyNames is considerably larger than in recent data collections in L\&V.
Moreover, our images show objects in complex visual contexts,
unlike the ``clean'' ImageNet data~\cite{imagenet_cvpr09} that is popular in Computer Vision \cite{ILSVRC15}, and unlike stylized line drawings used in picture naming experiments in Cognitive Science (Figure \ref{fig:cake}).

The trends we identify in the dataset are illustrated in Figure \ref{fig:cake} (left): Our data reveals clear naming preferences (in the example, 53\% of the annotators prefer the so-called basic-level name \textit{cake}, see Section \ref{subsec:rosch} for further explananation) and also rich variation (the remaining annotators prefer other options like \textit{food, dessert, bread}) that is not restricted to taxonomic (i.e.,\ hierarchical) relations studied in previous work on naming \cite{Ordonez:2016,graf2016animal}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "lrec2020naming"
%%% End:
