\begin{table*}
	\centering
	\small
	\begin{tabular}{l@{~}|@{~}r@{~}r@{~}rr@{~}|@{~}r@{~}r@{~}rr@{~}|@{~}r@{~}r@{~}rr}
		\toprule
		%{} &  All images-top=VG &  All images-top=MN-1 &  All images-top\$\textbackslash in\$MN &  All images-KL &  VGnotMN-top=VG &  VGnotMN-top=MN-1 &  VGnotMN-top\$\textbackslash in\$MN &  VGnotMN-KL &  notTrain-top=VG &  notTrain-top=MN-1 &  notTrain-top\$\textbackslash in\$MN &  notTrain-KL \\
		& \multicolumn{12}{c}{Image subset} \\
		&	\multicolumn{4}{c}{All ($24,585$)} 
		& \multicolumn{4}{c}{VG$\neq$MN-1 ($6,280$)}
		& \multicolumn{4}{c}{$\neg$Training ($2,281$)} \\
		\midrule
		&  \multicolumn{2}{c}{single-label}
		&  \multicolumn{2}{c}{multi-label}
		&  \multicolumn{2}{c}{single-label}
		&  \multicolumn{2}{c}{multi-label}
		&  \multicolumn{2}{c}{single-label}
		&  \multicolumn{2}{c}{multi-label} \\
		Domain	 &  =VG & =MN-1 & $\in$MN  &  KL
		&  =VG & =MN-1 & $\in$MN  & KL
		&  =VG & =MN-1 & $\in$MN  & KL\\
		\midrule
		all            &               68.7 &                 76.8 &                   94.8 &            0.9 &            20.7 &              52.6 &                92.4 &         1.2 &             63.5 &               73.1 &                 92.5 &          1.0 \\ \\
		home           &               70.0 &                 74.7 &                   93.4 &            1.1 &            24.3 &              47.5 &                91.4 &         1.4 &             67.2 &               70.5 &                 92.5 &          1.2 \\
		food           &               60.0 &                 71.9 &                   87.7 &            1.0 &            14.2 &              48.9 &                76.6 &         1.5 &             46.6 &               62.1 &                 80.1 &          1.3 \\
		buildings      &               57.5 &                 70.7 &                   92.1 &            1.3 &             7.9 &              63.4 &                89.8 &         1.5 &             44.3 &               60.2 &                 88.6 &          1.4 \\
		vehicles       &               69.8 &                 72.8 &                   97.6 &            0.6 &            35.3 &              46.1 &                97.4 &         0.7 &             67.2 &               73.5 &                 96.4 &          0.7 \\
		animals/plants &               92.7 &                 94.8 &                   97.7 &            0.4 &            20.3 &              65.0 &                94.0 &         0.9 &             91.3 &               92.6 &                 96.2 &          0.5 \\
		clothing       &               63.3 &                 70.9 &                   92.6 &            1.0 &            19.9 &              48.5 &                90.3 &         1.2 &             50.8 &               60.2 &                 87.4 &          1.3 \\
		people         &               48.5 &                 70.7 &                   95.5 &            1.2 &            13.8 &              58.9 &                95.3 &         1.3 &             42.2 &               68.2 &                 92.8 &          1.2 \\
		\bottomrule
		%Baseline      &                3.3 &                  5.2 &                   12.0 &            -- &             1.7 &               9.3 &                21.8 &         -- &              3.0 &                5.0 &                 11.9 &          -- \\
		Baseline       &                3.9 &                  4.6 &                    5.6 &            -- &             0.1 &               3.0 &                 3.8 &         -- &              3.8 &                4.6 &                  5.8 &          -- \\
	\end{tabular}
	\caption{Baseline: The most frequent name. Random baseline (KL with random distributions) for each image set: 6.9 (all), 6.5 (VG$\neq$MN-1) and 6.9 ($\neg$Training). The numbers in parentheses give the \# instances in each set.	\label{tab:model}}
\end{table*}
%
% mismatch between the visual awareness of the target object (man) and that of the competitor object (shirt)
\cs{Ah, we still should have a comparison model with above argumentation :/.}
%
We use ManyNames\ (MN) as evaluation data, and assess whether an object recognition model can account for MN's naming variants provided for individual objects. 
In contrast to existing works on multi-label classification, which also train their models towards multi-label predictions 
% multi-label data also during training 
(cf.\ Section\ \ref{sec:relwork}\cs{check that this is said there}), we use a model that was trained on single names---VG's object names (and attributes, though). %, i.e., it did not see multiple names for individual objects during training.  
Through this case study we examine whether classification models are able to implicitly learn representations of the relations between object names despite of not being trained towards this. 
We argue that this is a more natural setting, since people may not hear different names when referring to the same object, but rather across different objects of the same class \cs{any REF to paper where name choice for specific object stays consistent once done?}. 
%By this means, we examine whether classification models are able to implicitly learn representations of the relations between object names despite of not being trained towards this. 
And due to the nature of our collected data, the relations may not stand in a hierarchical relation. 

%We hypothesize that this is the case, see finding regarding the influence of visual cues. 
%Get an intuition in how far models are able to identify an object by one of it's possible names (gt=dog, is system still able to identify the object if the name is puppy?).
\cs{And with this argumentation, we'd ideally compare to a model fine-tuned on our data ... and to a model trained on all our names with a response count$>$n ... so much future work}


\subsection{Experimental Setup}
\textbf{STILL TO BE DONE: Model, Data}
\paragraph{Classification model}
We used \citeauthor{anderson2018updown}'s \citeyear{anderson2018updown} bottom-up attention model (\textit{Bottom-up} henceforth)\footnote{The features code and data is available at \url{https://github.com/peteanderson80/bottom-up-attention}}. 
It uses Faster R-CNN in conjunction with the ResNet-101 CNN and was pretrained on object and attribute annotations from VisualGenome (after initialization with pretrained ImageNet model).
\iffalse
"To pretrain the bottom-up attention model, we first initialize Faster R-CNN with ResNet-101 pretrained for classification on ImageNet [35]. We then train on Visual
Genome [21] data. To aid the learning of good feature
representations, we add an additional training output for
predicting attribute classes (in addition to object classes).
To predict attributes for region i, we concatenate the mean
pooled convolutional feature vi with a learned embedding
of the ground-truth object class, and feed this into an additional output layer defining a softmax distribution over each
attribute class plus a ‘no attributes’ class.
The original Faster R-CNN multi-task loss function contains four components, defined over the classification and
bounding box regression outputs for both the RPN and the
final object class proposals respectively. We retain these
components and add an additional multi-class loss component to train the attribute predictor"
\fi

\paragraph{Data}
\begin{itemize}
	\item We use the softmax distribution for the VG object names, and ignore the distribution over attributes. 
	\item The authors manually cleaned the set of object names to remove those with poor detection performance.
	\item Set of classes: 1,600 object names
	Overlapping classes, such as \textsl{person} and \textsl{guy}, were not merged to single classes. (For those cases, we therefore would expect that evaluating the model output on our set of names per instance to yield higher detection accuracy. )
	\item Image set and vocabulary: $346$~name types are not in MN, i.e., our vocabulary (MN--VG) contains $1,254$~names. 
	We filtered out those images whose VG name was not found in MN--VG, which resulted in $24,585$~instances (note that of those $22,304$~images were also part of the training data of bottom-up)
\end{itemize} 

\paragraph{Evaluation measures}
Model effectiveness is assessed in both, the commonly used single-ground-truth setting and in a multi-label setting. 
In the former, for each instance, we compare the model's top prediction to its VG name and to the top response in MN, respectively, and report the average accuracy across all instances (\textit{=VG} and \textit{=MN-1}, respectively).
In the multi-label setting, we report the average accuracy of the top prediction to match any of the responses in MN. 
To assess the model's ability to account for human naming variations of MN instances, including the likelihood of names being produced for an instance, we treat the model's softmax output~$S$ as a probability distribution and compare it to the probability distribution~$M$ over the MN responses (estimated by normalizing the name counts). 
We report the average Kullback-Leibler divergence~$\mathrm{D_{KL}}(M||S)$ \cite{kullback1951information}. 

\iffalse
\textsl{top}: the model's top prediction
\begin{itemize}
	\item \textbf{=VG} The average model accuracy of \textsl{top} on the VG names
	\item \textbf{=MN-1} The average model accuracy of \textsl{top} on the top response in ManyNames
	\item \textbf{$\in$MN} The average model accuracy of \textsl{top} to match any name of the responses in ManyNames
	\item \textbf{KL} The average Kullback-Leibler divergence~$\mathrm{D_{KL}}(M||S)$ of the model's softmax output~$S$ and the  probability distribution~$M$ over ManyNames, estimated by normalizing the name counts per instance
\end{itemize}
\fi

\paragraph{Results \& Discussion} 
We compare Bottom-up against a random baseline for the KL divergence, and against a most-frequent name baseline \cs{not sure whether the latter, at least in its current form, makes sense. TBD?}. 
Apart from the evaluation on the full set of instances (\textit{\mbox{All}} in Table~\ref{tab:model}), we further assess model effectiveness on the instances whose  \vg names do not match the top response in MN (\mbox{\textit{VG}$\neq$\textit{MN-1}}), and on those which were not part of the set of images used for training bottom-up (\mbox{$\neg$\textit{Training}}).
Note that the latter two sets are disjoint. 
%

The model performs worse on VG than on the top MN response (MN-1), even though the model saw the majority of the instances during training. 
It achieves a higher accuracy of up to\ 55\% points on instances for which MN-1 and the VG name differ (\textsc{buildings}, VG$\neq$MN-1). 
The reasons may be that MN-1 is more general than VG for many cases (e.g.,\ the examples for \textsl{batter} in Fig.\ \ref{fig:ex-high-low-agreement}), or that MN-1 is more reliable due to up to $36$~annotations per instance as well as to our elicitation procedure\footnote{Recall that we instructed the workers explicitly to produce object names, and did not obtain them by canonicalizing object descriptions as was done for VG.} (e.g.,\ (\textsl{sandwich,hotdog}), Fig.\ \ref{fig:ex-high-low-agreement}). 
Another reason could be that the model reproduces naming choices ("errors") caused by cases of visual uncertainty (cf.\ Section\ \ref{ssec:variation}; e.g.,\ (\textsl{bridge,street})). 

In the multi-label setting, $\in$MN, the difference in accuracy between the objects in All and in the subset VG!=MN-1 is much lower compared to the MN-1 setting ($-2.5$\% vs.\ $-24.2$\%\ points, all domains). 
Comparison to the results on unseen images demonstrates Bottom-up's generalization ability. 
While the difference varies highly across the domains (All vs. \mbox{$\neg$\textit{Training}}), it is negligible on average (row \textit{all}), and the same on the \mbox{VG!=MN-1} images. 
%Unsurprisingly, the effectiveness of Bottom-up drops on the images which it has not seen during training, and the difference in accuracy highly varies across the domains (All vs. \mbox{$\neg$\textit{Training}}). 
Taken together, we thus believe that our ManyNames dataset provides a more robust evaluation framework than single-label datasets, since a model's effectiveness is not measured by a single naming choice. 

The dataset furthermore allows for comparing models on weighted naming alternatives, as illustrated by measuring the KL divergence. 
As shown in the table, the model outperforms the random baseline on all image sets. \cs{we could also (better?) use the average responses over all instances in MN -- TBD}
It best accounts for human naming choices on the \textsc{animals\_plants} and \textsc{vehicles} domains (for which we found workers to give more general names, have the least number of types per object, and the highest agreement\ $H$, Tab.\ \ref{tab:agree}), and worst on \textsc{food} and \textsc{buildings}, which tend to contain occluded or unclear bounding boxes \cs{can we say that?}. 
%For these more difficult instances (VG!=MN-1),for multi-label is much lower to All (columns inMN) $\Rightarrow$ MN provides a more robust evaluation framework; highest drop in food domain

Comparing the model output to MN on individual images gives an intuition for how much Bottom-up accounts for naming choices due to different sources of variation: 
In some cases, the model does not make clear referential mistakes made by workers (e.g.,\ \textsl{(boy,helmet)},\ Fig.\ \ref{fig:ex-high-low-agreement}), but overall tends to mirror  cases of human "referential uncertainty" 
%(\textsc{clothing}-\textsc{person}: 
(\textsl{(man,shirt)}, \textsl{(bed,sheet)}, but see \textsl{(book,bed})), as well as cross-classification (\textsl{(boy,batter)}, \textsl{(sandwich,food)}), and metonymy \textsl{(food,basket/plate)}. 
It reflects perceptual mistakes caused by unclear images (\textit{sandwich,banana}), but makes also quite a few predictions which are unreasonable or inconsistent from a human perspective (and not contained in MN), such as \textsl{(bridge,sky)}, \textsl{(pants/player,dirt)}. 

\iffalse
\begin{itemize}
	\item VG worse than MN; considering that model was trained on VG - is it because MN has easier labels? Does this mean that generalisation ability is better evaluated on MN?
	\item VG!=MN-1 much worse than All $\Rightarrow$ Former has more difficult/ambiguous images (for both? classifier and human?)
	\item for those more difficult images (VG!=MN-1), the difference for multi-label is much lower to All (columns inMN) $\Rightarrow$ MN provides a more robust evaluation framework; highest drop in food domain
	\item accuracy drops for notTrain (except for vehicles MN-1), but is still much better than VG!=MN-1  
\end{itemize}
 
\paragraph{Error Analysis}
\begin{itemize}
	\item Model does not make clear mistakes workers did, e.g. \textsl{boy-helmet} in Table\ \ref{fig:ex-high-low-agreement}; or spatial relationships (book-bed) ("referential uncertainty")
	\item mirrors human "referential uncertainty" mistakes for clothing-person errors (man,shirt); and difficult.to.identify objects (bed,sheet)
	\item visual perceptual errors: banana (human and model); but more than humans (...)
	\item but makes clear, unreasonable mistakes (dirt, ...; bed,table; sky)
	\item cross-classification: great (boy,batter; sandwich,food)
	\item metonymy: (food,basket/plate)
\end{itemize}
\fi
