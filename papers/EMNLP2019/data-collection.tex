We take data from \vgenome \cite{krishna2016visualgenome}, which aims to provide a full set of descriptions of the scenes which images depict in order to spur complete scene understanding. 
It contains a dense region-based labeling of $108k$~images with textual expression of the attributes and references of objects, their relationships as well as question answer pairs, all linked to WordNet synsets \cite[see below]{fellbaum1998wordnet}.

% \gbt{START to be refined -- taken from sivl submission as is}

% \vgenome \cite{krishna2016visualgenome} aims to provide a full set of descriptions of the scenes which images depict in order to spur complete scene understanding. 
% It contains a dense region-based labeling of $108k$~images with textual expression of the attributes and references of objects, their relationships as well as question answer pairs, all linked to WordNet synsets \cite[see below]{fellbaum1998wordnet}. 


% \begin{itemize}
%      		\item[(1)] \textbf{Specific categories}: are not available, as object categories and names are not consistently annotated (and even conflated)
% 				\item[(2)] \textbf{Exhaustive annotations}: are available, which is a huge advantage of this data sets
% 		   \item[(3)] \textbf{Natural names}: are available, though object names might not be fully discriminative (as in referring expressions)

% \end{itemize}
% \gbt{END to be refined -- taken from sivl submission as is}

\subsection{Sampling of Instances (Images/Objects)}

We aimed at collecting a relatively large amount of naturalistic images (\textit{instances}) that depict common objects.
 % of frequent classes/names in  \vgenome, which, at the same time, have been frequently/commonly studied in the psycholinguistic literature. 
%Criteria: From CV: select images depicting objects with relatively frequent names; From CogSci: select objects which have been frequently studied in cognitive science/psychological norming studies; we chose McRae et al. as basis.
We start from the concepts of McRae et al.'s feature norms (REF), which are common objects of different categories (e.g.,~\textsc{animals}, \textsc{furniture}) and, as such, have a high overlap with standard datasets of object norming studies (REFS).
We added the \textsc{person} category because it is very frequent category in \vgenome.

(As appropriate: We use image and object interchangeably in the following, since we only selected one target object per image (i.e., each object and image in VG is chosen at most once).)

\paragraph{Collection nodes}
% Standard taxonomies do not align well with name variability; for instance, people use more varied names for types of dogs than for types of cows, while both dogs and cows are mammals.
We defined a set of \textit{collection nodes} which we would then use to collect our object instances from VG. 

We based the definition of our set of nodes on the WN (REF) synsets of the McRae concepts (e.g.,~dog, duck, goose, gull), the nominal WordNet hierarchy, and the frequency distribution of the VG object names' synsets.\footnote{TODO: need to be clear from the general description of VG that the frequ. of instances labeled with the synset of the object name is meant.} 

First, we selected a set of collection node candidates---synsets which match (e.g.,~\textsl{dog, duck, goose, gull}) or subsume (e.g.,~\textsl{mammal, bird}) the McRae synsets\footnote{Specific synset IDs, e.g.,~dog.n.01, are omitted for readability.}. 
From these candidates we kept those as collection nodes which had a high frequency of VG object instances of different names. For example, VG instances  subsumed by McRae's \textsl{dog} were named \textsl{beagle, greyhound, puppy, bulldog}, etc., while McRae's \textsl{duck, goose}, or \textsl{gull} did not have name variants in VG, so we kept \textsl{dog} and \textsl{bird} as collection nodes.

\paragraph{Collection of instance candidates}
Goal of above procedure was the collection of instances of selected object classes---our nodes--- whose VG names correspond to or subsume (are hypernyms of) a McRae concept, and whose object names differ, that is, of which we can expect that people possess different names for them (e.g.,~\textsl{duck, goose, gull} for \textsl{bird}).
The collection of such instances using the nodes was then straightforward: We retrieved all VG images  depicting an object whose name matches or is subsumed by one of the collection nodes. 
We did not consider objects with names in plural form, with parts-of-speech other than nouns\footnote{(REF to tagger)}, or that were multi-word expressions/phrases (e.g.,~\textsl{pink bird}). 
We further only considered objects whose bounding box\footnote{TODO: need to be clear from the general description of VG what is meant.} had an area of~$20-90\%$ of the whole image area.

\paragraph{Sampling of instances}
Finally, from this set of instances we sampled our final dataset of $31,093$~instances. 
Sampling proceeded in dependence on the overall size of the individual collection seeds: 
up to~$800$ objects per seed: all instances, but at most 500, are collected; more than~$800$ objects per seed: all instances, but at most~$1,000$, are collected.  \textbf{double-check}

\cs{END @ GBT}

Table~\ref{tab:XXX} gives an overview of the collection nodes, XXX, XXX, grouped into $7$~domains. (\textbf{Report only dataset after round0, with a note in caption/footnote referring to the checkpoint pruning.})
\gbt{I would put only a table with the final dataset, not the intermediate one (round0). That is, include a table with the 25K imgs (the 30K imgs were just our initial pool).}

\begin{table*}
\begin{tabular}{@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l}
	\toprule
	          vehicles &            food & animals\_plants &           home &        buildings &             people &      clothing \\
	\midrule
	  train (954) &  pizza (518) &  giraffe (915) &  bed (888) &  house (340) &  boy (853) &  shirt (904) \\
	  car (642) &  cake (261) &  horse (822) &  bench (714) &  bridge (274) &  man (806) &  jacket (451) \\
	  plane (485) &  bread (186) &  cat (754) &  table (687) &  dugout (91) &  woman (766) &  coat (267) \\
	  airplane (479) &  sandwich (153) &  dog (654) &  desk (672) &  tent (53) &  girl (650) &  dress (190) \\
	  motorcycle (466) &  bun (143) &  zebra (461) &  counter (516) &  restaurant (33) &  lady (342) &  hat (77) \\
	  truck (465) &  cheese (110) &  cow (324) &  couch (366) &  overpass (23) &  guy (330) &  t-shirt (62) \\
	 boat (450) &  donut (78) &  bird (295) &  chair (365) &  grill (22) &  child (230) &  tie (51) \\
	  jet (106) &  salad (70) &  sheep (216) &  carpet (307) &  garage (18) &  batter (110) &  blazer (43) \\
	  aircraft (85) &  sauce (68) &  bull (48) &  bowl (219) &  hotel (16) &  kid (85) &  hood (26) \\
	  van (76) &  apple (33) &  flower (40) &  curtain (182) &  castle (14) &  skateboarder (80) &  cap (20) \\
	\bottomrule
\end{tabular}
	\caption{Overview of our dataset: Top-10 VG names for each domain (number of instances in parentheses). \textbf{double-check} \label{tab:overview_dataset1}}
\end{table*}

\begin{table*}
\small
	\begin{tabular}{@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l}
	\toprule
	        animals\_plants &               vehicles &                        home &                clothing &                   buildings &                    food &                 people \\
	\midrule
	  ungulate.n.01 (2037) &  aircraft.n.01 (1208) &  furnishing.n.02 (5355) &  shirt.n.01 (968) &  house.n.01 (364) &  dish.n.02 (812) &  woman.n.01 (1768) \\
	 horse.n.01 (833) &  train.n.01 (957) &  vessel.n.03 (525) &  overgarment.n.01 (786) &  bridge.n.01 (297) &  baked\_goods.n.01 (770) &  man.n.01 (1167) \\
	  feline.n.01 (763) &  car.n.01 (727) &  kitchen\_utensil.n.01 (132) &  dress.n.01 (199) &  shelter.n.01 (169) &  foodstuff.n.02 (280) &  male\_child.n.01 (853) \\
	 dog.n.01 (688) &  motorcycle.n.01 (564) &  crockery.n.01 (92) &  headdress.n.01 (135) &  restaurant.n.01 (58) &  vegetable.n.01 (48) &  athlete.n.01 (396) \\
	  bird.n.01 (389) &  truck.n.01 (559) &  cutlery.n.02 (82) &  neckwear.n.01 (65) &  outbuilding.n.01 (31) &  edible\_fruit.n.01 (42) &  child.n.01 (333) \\
	  flower.n.01 (44) &  boat.n.01 (499) &  tool.n.01 (72) &  robe.n.01 (27) &  hotel.n.01 (19) &  beverage.n.01 (23) &  creator.n.02 (11) \\
	  rodent.n.01 (27) &  ship.n.01 (38) &  lamp.n.01 (34) &  glove.n.02 (7) &  housing.n.01 (17) &   &  professional.n.01 (5) \\
	 insect.n.01 (12) &   &   &  footwear.n.01 (5) &  place\_of\_worship.n.01 (12) &   &   \\
	  fish.n.01 (11) &   &   &   &   &   &   \\
	\bottomrule
\end{tabular}
	\caption{Overview of our dataset: Collection nodes for each domain (number of instances in parentheses). \textbf{double-check} \label{tab:overview_dataset2}}
\end{table*}

% \begin{itemize}
% \item rows: domains (if we go for long paper: then one row per collection node?)
% \item columns:
%   \begin{enumerate}
%   \item \# collection nodes
%   \item collection nodes (list)
%   \item \# unique VG names
%   \item example VG names
%   \item \# unique objects
%   \item \# unique images (? not sure if necessary; maybe only one of unique {objects, images})
%   \end{enumerate}
% \end{itemize}

Number of images/objects:        25,596\\
Number of object names:  450\\
Number of collection nodes (synsets):    52 \\

\subsection{Procedure} describe the crowdsourcing set-up and the task
TODO: Footnote: we ran pilot experiments to design our experiment and instructions.
\paragraph{Collection Method}
\begin{itemize}
	\item instructions; \\
	Appendix~\ref{app:instructions} gives the instructions as they were presented to the annotators. 
	\item each round: HIT of 10 instances, collect 9~annotations for each HIT
	\item round 0 (with opt-outs) $\rightarrow$ pruning $\rightarrow$ rounds 1-3 (no opt-outs)\\
	pruning: Based on given opt-outs: keep images with no OCCLUSION, at most BBOX is ambiguous twice, at most 17\% of names in plural form, most frequent names is of same domain as VG name (gives $25,596$, i.e., discard $5,497$ instances)
	\item workers could only participate in one round, such as to avoid workers annotating an instance more than once. 
\end{itemize}

Overall $XX$~participants, each annotated between $XX$ and $XX$ instances. \gbt{Put mean or median instead of min-max}

\subsection{Data} 

give an overview of the collected data

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
