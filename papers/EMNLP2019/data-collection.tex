We take data from \vgenome \cite{krishna2016visualgenome}, which
% aims to provide a full set of descriptions of the scenes which images depict in order to spur complete scene understanding. 
contains a dense region-based labeling of $108k$~images with object descriptions, attributes, and relationships, as well as question-answer pairs, all linked to WordNet synsets \cite{fellbaum1998wordnet}.
\vgenome is suitable for our purpose of collecting names for a relatively large amount of instances of common objects in
naturalistic images, as it has images of varying complexity, with close-ups as well as complex images with many objects.
As common in Computer Vision, objects are identified via bounding boxes (see red boxes in Figure~\ref{fig:cake}).% 
\footnote{We use image and object interchangeably in the following, since we only selected one target object per image (i.e., each object and image in VG is chosen at most once).}

\subsection{Sampling of Instances}

 % of frequent classes/names in  \vgenome, which, at the same time, have been frequently/commonly studied in the psycholinguistic literature. 
%Criteria: From CV: select images depicting objects with relatively frequent names; From CogSci: select objects which have been frequently studied in cognitive science/psychological norming studies; we chose McRae et al. as basis.
We selected seven domains: six from McRae et al.'s feature norms (REF), a dataset widely used in Psycholinguistics that consists of common objects of different categories (e.g.,~\textsc{animals}, \textsc{furniture}), and \textsc{person}, because it is very frequent category in \vgenome.
% We start from the concepts of McRae et al.'s feature norms (REF), which are common objects of different categories (e.g.,~\textsc{animals}, \textsc{furniture}) and, as such, have a high overlap with standard datasets of object norming studies (REFS).
% We added the \textsc{person} category because it is very frequent category in \vgenome.

Within each domain, we aimed at collecting instances at different levels in a taxonomy to cover a wide range of phenomena, but it is not straightforward to do so because ontological taxonomies do not align well with the lexicon (for instance, \textit{dog} and \textit{cow} are both mammals, but \textit{dog} has many more common subcategories), and most domains are not organized in a clear taxonomy in the first place (e.g.\ \textsc{home}).
% Standard taxonomies do not align well with name variability; for instance, people use more varied names for types of dogs than for types of cows, while both dogs and cows are mammals.
Instead, we defined a set of synsets that we would use to collect our object instances from \vgenome, balancing variability.
From the set of synsets that match or subsume the concepts in the McRae norms, we kept those that had a high number of \vgenome object instances of different names.
For example, \vgenome instances subsumed by McRae's \textsl{dog} were named \textsl{beagle, greyhound, puppy, bulldog}, etc., while McRae's \textsl{duck}, \textsl{goose}, or \textsl{gull} did not have name variants in \vgenome, so we kept \textsl{dog} and \textsl{bird} (which subsumes \textsl{duck}, \textsl{goose}, or \textsl{gull}) as collection synsets.

We then retrieved all VG images depicting an object whose name matches or is subsumed by words in one of these synsets; we refer to these words as \textit{seeds}, and we had XXX\gbt{Carina?} of them.
We did not consider objects with names in plural form, with parts-of-speech other than nouns\footnote{(REF to tagger)}, or that were multi-word expressions (e.g.,~\textsl{pink bird}). 
We further only considered objects whose bounding box covered an area of~$20-90\%$ of the image.
% We based the definition of our set of nodes on the WN (REF) synsets of the McRae concepts (e.g.,~dog, duck, goose, gull), the nominal WordNet hierarchy, and the frequency distribution of the VG object names' synsets.\footnote{TODO: need to be clear from the general description of VG that the frequ. of instances labeled with the synset of the object name is meant.} 

% First, we selected a set of collection node candidates---synsets which match (e.g.,~\textsl{dog, duck, goose, gull}) or subsume (e.g.,~\textsl{mammal, bird}) the McRae synsets\footnote{Specific synset IDs, e.g.,~dog.n.01, are omitted for readability.}. 
% From these candidates we kept those as collection nodes which had a high frequency of VG object instances of different names. For example, VG instances  subsumed by McRae's \textsl{dog} were named \textsl{beagle, greyhound, puppy, bulldog}, etc., while McRae's \textsl{duck, goose}, or \textsl{gull} did not have name variants in VG, so we kept \textsl{dog} and \textsl{bird} as collection nodes.

%\paragraph{Collection of instance candidates}
% Goal of above procedure was the collection of instances of selected object classes---our nodes--- whose VG names correspond to or subsume (are hypernyms of) a McRae concept, and whose object names differ, that is, of which we can expect that people possess different names for them (e.g.,~\textsl{duck, goose, gull} for \textsl{bird}).

% \paragraph{Sampling of instances}

Because of the Zipfian distribution of names, and to balance the collection, we sampled instances depending on the size of the seeds: up to 500 instances for seeds with up to 800 objects, and up to 1000 instances for larger seeds. \textbf{double-check}
This yielded a dataset with $31,093$~instances, which was further pruned during annotation (see next subsection).

\gbt{Checked up to here}.

Table~\ref{tab:XXX} gives an overview of the collection synsets, XXX, XXX, grouped into $7$~domains. (\textbf{Report only final dataset, with a note in caption/footnote referring to the checkpoint pruning.})

\begin{table*}
\begin{tabular}{@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l}
	\toprule
	          vehicles &            food & animals\_plants &           home &        buildings &             people &      clothing \\
	\midrule
	  train (954) &  pizza (518) &  giraffe (915) &  bed (888) &  house (340) &  boy (853) &  shirt (904) \\
	  car (642) &  cake (261) &  horse (822) &  bench (714) &  bridge (274) &  man (806) &  jacket (451) \\
	  plane (485) &  bread (186) &  cat (754) &  table (687) &  dugout (91) &  woman (766) &  coat (267) \\
	  airplane (479) &  sandwich (153) &  dog (654) &  desk (672) &  tent (53) &  girl (650) &  dress (190) \\
	  motorcycle (466) &  bun (143) &  zebra (461) &  counter (516) &  restaurant (33) &  lady (342) &  hat (77) \\
	  truck (465) &  cheese (110) &  cow (324) &  couch (366) &  overpass (23) &  guy (330) &  t-shirt (62) \\
	 boat (450) &  donut (78) &  bird (295) &  chair (365) &  grill (22) &  child (230) &  tie (51) \\
	  jet (106) &  salad (70) &  sheep (216) &  carpet (307) &  garage (18) &  batter (110) &  blazer (43) \\
	  aircraft (85) &  sauce (68) &  bull (48) &  bowl (219) &  hotel (16) &  kid (85) &  hood (26) \\
	  van (76) &  apple (33) &  flower (40) &  curtain (182) &  castle (14) &  skateboarder (80) &  cap (20) \\
	\bottomrule
\end{tabular}
	\caption{Overview of our dataset: Top-10 VG names for each domain (number of instances in parentheses). \textbf{double-check} \label{tab:overview_dataset1}}
\end{table*}

\begin{table*}
\small
	\begin{tabular}{@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l@{~}l}
	\toprule
	        animals\_plants &               vehicles &                        home &                clothing &                   buildings &                    food &                 people \\
	\midrule
	  ungulate$_1$ (2037) &  aircraft$_1$ (1208) &  furnishing$_2$ (5355) &  shirt$_1$ (968) &  house$_1$ (364) &  dish$_2$ (812) &  woman$_1$ (1768) \\
	 horse$_1$ (833) &  train$_1$ (957) &  vessel$_3$ (525) &  overgarment$_1$ (786) &  bridge$_1$ (297) &  baked\_goods$_1$ (770) &  man$_1$ (1167) \\
	  feline$_1$ (763) &  car$_1$ (727) &  kitchen\_utensil$_1$ (132) &  dress$_1$ (199) &  shelter$_1$ (169) &  foodstuff$_2$ (280) &  male\_child$_1$ (853) \\
	 dog$_1$ (688) &  motorcycle$_1$ (564) &  crockery$_1$ (92) &  headdress$_1$ (135) &  restaurant$_1$ (58) &  vegetable$_1$ (48) &  athlete$_1$ (396) \\
	  bird$_1$ (389) &  truck$_1$ (559) &  cutlery$_2$ (82) &  neckwear$_1$ (65) &  outbuilding$_1$ (31) &  edible\_fruit$_1$ (42) &  child$_1$ (333) \\
	  flower$_1$ (44) &  boat$_1$ (499) &  tool$_1$ (72) &  robe$_1$ (27) &  hotel$_1$ (19) &  beverage$_1$ (23) &  creator$_2$ (11) \\
	  rodent$_1$ (27) &  ship$_1$ (38) &  lamp$_1$ (34) &  glove$_2$ (7) &  housing$_1$ (17) &   &  professional$_1$ (5) \\
	 insect$_1$ (12) &   &   &  footwear$_1$ (5) &  place\_of\_worship$_1$ (12) &   &   \\
	  fish$_1$ (11) &   &   &   &   &   &   \\
	\bottomrule
\end{tabular}
	\caption{Overview of our dataset: Synset nodes for each domain (subscript indicates synset number; number of instances in parentheses). \textbf{double-check} \label{tab:overview_dataset2}}
\end{table*}

% \begin{itemize}
% \item rows: domains (if we go for long paper: then one row per collection node?)
% \item columns:
%   \begin{enumerate}
%   \item \# collection nodes
%   \item collection nodes (list)
%   \item \# unique VG names
%   \item example VG names
%   \item \# unique objects
%   \item \# unique images (? not sure if necessary; maybe only one of unique {objects, images})
%   \end{enumerate}
% \end{itemize}

Number of images/objects:        25,596\\
Number of object names:  450\\
Number of collection nodes (synsets):    52 \\

\subsection{Procedure} describe the crowdsourcing set-up and the task
TODO: Footnote: we ran pilot experiments to design our experiment and instructions.
\paragraph{Collection Method}
\begin{itemize}
	\item instructions; \\
	Appendix~\ref{app:instructions} gives the instructions as they were presented to the annotators. 
	\item each round: HIT of 10 instances, collect 9~annotations for each HIT
	\item round 0 (with opt-outs) $\rightarrow$ pruning $\rightarrow$ rounds 1-3 (no opt-outs)\\
	pruning: Based on given opt-outs: keep images with no OCCLUSION, at most BBOX is ambiguous twice, at most 17\% of names in plural form, most frequent names is of same domain as VG name (gives $25,596$, i.e., discard $5,497$ instances)
	\item workers could only participate in one round, such as to avoid workers annotating an instance more than once. 
\end{itemize}

Overall $XX$~participants, each annotated between $XX$ and $XX$ instances. \gbt{Put mean or median instead of min-max}

\subsection{Data} 

give an overview of the collected data

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
